# Using AWS S3 services on our local machine

1. Create an instance and ssh into the instance
2. Install Python
```sudo apt install python```
3. Install pip
```sudo apt install python3-pip```

4. Tells linux to use python3
```alias python=python3```
5. Install the AWS Command Line Interface:
```sudo python3 -m pip install awscli```
6. Now we configure AWS in our local machine 
```aws configure``` 
7. We will be promted for our access keys and secret key which should be provided to you.


![key prompt](https://user-images.githubusercontent.com/129324316/234842151-1aab5844-2020-4528-9467-92ae4407b6ff.png)


8. Enter `access key` and `private key`
9. Enter region name `eu-west-1`
10. default output `json`
11. S3 will recognise the keys and allow us access s3
12. If it says permission denied, you may have entered incorrectly, in this case run `aws configure` again.
13. Now we access aws and access the s3 service and we name the cloud provider or name the command 
```aws s3 ls```
14. Should list contents of s3
15. Make Bucket usign naming convention:
``` aws s3 mb s3://fatima-tech221```
16. Confirm with aws using `aws s3 ls` where you should see your bucket present
17. We can transfer/migrate data using aws cli
18. Create file
```sudo nano test.txt```
19. Confirm:
```cat test.txt```
20. Now we can migrate the file from our ec2 instance to the bucket we created:
```aws s3 cp test.txt s3://fatima-tech221```
21. To delete the file 
```ubuntu@ip-172-31-49-97:~$ sudo rm test.txt```

22. To download our copied file, we must do
```aws s3 cp s3://fatima-tech221/test.txt /home/ubuntu```
In order to retrieve data^^


# Boto3 Implementation
Boto3 Implemetation 

1. Create and SSH into an ec2 instance
2. Make dure to update and upgrade
3. Install python3 and pip 
4. Install Boto3 and aws cli ```sudo python3 -m pip install boto3```
5. Then we use aws configure and enter our access keys 
6. Specify the region and output format
7. Create a python file and input the following script to create a bucket using boto3:

``` 
import boto3


client = boto3.client('s3')

s3 = boto3.resource('s3')


AWS_REGION = "eu-west-1"



client = boto3.client("s3", region_name=AWS_REGION)



bucket_name = "fatima-tech221"

location = {'LocationConstraint': AWS_REGION}



response = client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)

print("Amazon S3 bucket has been created")

```
8. Now, we can upload data/files to the S3 bucket using python-boto3 by inserting the following into our scripts:


```
import boto3
s3 = boto3.resource('s3')

client = boto3.client('s3')

bucket_name="fatima-tech221"

s3.Bucket(bucket_name).upload_file('/home/ubuntu/test.txt','test.txt')
print('File Uploaded')
```

9. Now we can delete our file, and etrieve it from S3 using python-boto3
Using this:
```
import boto3


client = boto3.client('s3')
key = "test.txt"

s3 = boto3.resource('s3')



bucket_name="fatima-tech221"

s3.Bucket(bucket_name).download_file(key, "test.txt")

print("File downloaded")

```
10. We can also delete Content from S3 using python-boto3

```
import boto3


client = boto3.client('s3')
key = "test.txt"

s3 = boto3.resource('s3')
bucket_name="fatima-tech221"

s3.Object(bucket_name, 'test.txt').delete()




print('File Deleted')

```
11. We can now delete the whole bucket also:

```
import boto3


client = boto3.client('s3')
key = "test.txt"

s3 = boto3.resource('s3')
bucket=s3.Bucket("fatima-tech221")

bucket.delete




print('Bucket deleted')


```



